{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Natural Language Processing with Python â€“ Analyzing Text with the Natural Language Toolkit\n",
    "Steven Bird, Ewan Klein, and Edward Loper\n",
    "http://www.nltk.org/book/\n",
    "Chapter 7. Extracting Information from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.3 Developing and Evaluating Chunkers\n",
    "7.3.1 Reading IOB Format and the CoNLL 2000 Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt')[99])\n",
    "# (S\n",
    "#   (PP Over/IN)\n",
    "#   (NP a/DT cup/NN)\n",
    "#   (PP of/IN)\n",
    "#   (NP coffee/NN)\n",
    "#   ,/,\n",
    "#   (NP Mr./NNP Stone/NNP)\n",
    "#   (VP told/VBD)\n",
    "#   (NP his/PRP$ story/NN)\n",
    "#   ./.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])\n",
    "# (S\n",
    "#   Over/IN\n",
    "#   (NP a/DT cup/NN)\n",
    "#   of/IN\n",
    "#   (NP coffee/NN)\n",
    "#   ,/,\n",
    "#   (NP Mr./NNP Stone/NNP)\n",
    "#   told/VBD\n",
    "#   (NP his/PRP$ story/NN)\n",
    "#   ./.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.3.2 Simple Evaluation and Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  43.4%\n",
    "#     Precision:      0.0%\n",
    "#     Recall:         0.0%\n",
    "#     F-Measure:      0.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  87.7%\n",
    "#     Precision:     70.6%\n",
    "#     Recall:        67.8%\n",
    "#     F-Measure:     69.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_chunker = UnigramChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "print(unigram_chunker.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  92.9%\n",
    "#     Precision:     79.9%\n",
    "#     Recall:        86.8%\n",
    "#     F-Measure:     83.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                     for (word,pos) in sent.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_chunker.tagger.tag(postags))\n",
    "# [('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'),\n",
    "#  (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),\n",
    "#  ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),\n",
    "#  ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),\n",
    "#  ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),\n",
    "#  ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),\n",
    "#  ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),\n",
    "#  ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),\n",
    "#  ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),\n",
    "#  ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "print(bigram_chunker.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  93.3%\n",
    "#     Precision:     82.3%\n",
    "#     Recall:        86.8%\n",
    "#    F-Measure:     84.5%"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.3.3 Training Classifier-Based Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train(\n",
    "            train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f0e51cbfba26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     26\u001b[0m                          nltk.chunk.tree2conlltags(sent)]\n\u001b[1;32m     27\u001b[0m                         for sent in train_sents]\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunkTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.classifier = nltk.MaxentClassifier.train(\n\u001b[0;32m---> 13\u001b[0;31m             train_set, algorithm='megam', trace=0)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             return train_maxent_classifier_with_megam(\n\u001b[1;32m    299\u001b[0m                 \u001b[0mtrain_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 gaussian_prior_sigma, **cutoffs)\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tadm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcutoffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain_maxent_classifier_with_megam\u001b[0;34m(train_toks, trace, encoding, labels, gaussian_prior_sigma, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'-multilabel'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# each possible la\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'multiclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m     \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;31m# print './megam_i686.opt ', ' '.join(options)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;31m# Delete the training file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mcall_megam\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'args should be a list of strings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_megam_bin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mconfig_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Call megam via a subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[0;34m(bin)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MEGAM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'megam.opt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_686'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_i686.opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         url='http://www.umiacs.umd.edu/~hal/megam/index.html')\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e0154d4d574b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ChunkParse score:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     IOB Accuracy:  92.9%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     Precision:     79.9%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     Recall:        86.7%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunker' is not defined"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  92.9%\n",
    "#     Precision:     79.9%\n",
    "#     Recall:        86.7%\n",
    "#     F-Measure:     83.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f0e51cbfba26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     26\u001b[0m                          nltk.chunk.tree2conlltags(sent)]\n\u001b[1;32m     27\u001b[0m                         for sent in train_sents]\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunkTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.classifier = nltk.MaxentClassifier.train(\n\u001b[0;32m---> 13\u001b[0;31m             train_set, algorithm='megam', trace=0)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             return train_maxent_classifier_with_megam(\n\u001b[1;32m    299\u001b[0m                 \u001b[0mtrain_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 gaussian_prior_sigma, **cutoffs)\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tadm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcutoffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain_maxent_classifier_with_megam\u001b[0;34m(train_toks, trace, encoding, labels, gaussian_prior_sigma, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'-multilabel'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# each possible la\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'multiclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m     \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;31m# print './megam_i686.opt ', ' '.join(options)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;31m# Delete the training file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mcall_megam\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'args should be a list of strings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_megam_bin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mconfig_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Call megam via a subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[0;34m(bin)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MEGAM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'megam.opt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_686'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_i686.opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         url='http://www.umiacs.umd.edu/~hal/megam/index.html')\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-530a4ff4cc71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ChunkParse score:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     IOB Accuracy:  93.6%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     Precision:     81.9%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     Recall:        87.2%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunker' is not defined"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  93.6%\n",
    "#     Precision:     81.9%\n",
    "#     Recall:        87.2%\n",
    "#     F-Measure:     84.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f0e51cbfba26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     26\u001b[0m                          nltk.chunk.tree2conlltags(sent)]\n\u001b[1;32m     27\u001b[0m                         for sent in train_sents]\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunkTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.classifier = nltk.MaxentClassifier.train(\n\u001b[0;32m---> 13\u001b[0;31m             train_set, algorithm='megam', trace=0)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             return train_maxent_classifier_with_megam(\n\u001b[1;32m    299\u001b[0m                 \u001b[0mtrain_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 gaussian_prior_sigma, **cutoffs)\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tadm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcutoffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain_maxent_classifier_with_megam\u001b[0;34m(train_toks, trace, encoding, labels, gaussian_prior_sigma, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'-multilabel'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# each possible la\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'multiclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m     \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;31m# print './megam_i686.opt ', ' '.join(options)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;31m# Delete the training file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mcall_megam\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'args should be a list of strings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_megam_bin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mconfig_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Call megam via a subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[0;34m(bin)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MEGAM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'megam.opt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_686'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_i686.opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         url='http://www.umiacs.umd.edu/~hal/megam/index.html')\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f95b152ac1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ChunkParse score:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     IOB Accuracy:  94.5%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     Precision:     84.2%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     Recall:        89.4%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunker' is not defined"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  94.5%\n",
    "#     Precision:     84.2%\n",
    "#     Recall:        89.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = \"<END>\", \"<END>\"\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "    return {\"pos\": pos,\n",
    "            \"word\": word,\n",
    "            \"prevpos\": prevpos,\n",
    "            \"nextpos\": nextpos,\n",
    "            \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),\n",
    "            \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
    "            \"tags-since-dt\": tags_since_dt(sentence, i)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f0e51cbfba26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     26\u001b[0m                          nltk.chunk.tree2conlltags(sent)]\n\u001b[1;32m     27\u001b[0m                         for sent in train_sents]\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsecutiveNPChunkTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-bbbe91214224>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.classifier = nltk.MaxentClassifier.train(\n\u001b[0;32m---> 13\u001b[0;31m             train_set, algorithm='megam', trace=0)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             return train_maxent_classifier_with_megam(\n\u001b[1;32m    299\u001b[0m                 \u001b[0mtrain_toks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 gaussian_prior_sigma, **cutoffs)\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tadm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcutoffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/maxent.py\u001b[0m in \u001b[0;36mtrain_maxent_classifier_with_megam\u001b[0;34m(train_toks, trace, encoding, labels, gaussian_prior_sigma, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'-multilabel'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# each possible la\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'multiclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m     \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;31m# print './megam_i686.opt ', ' '.join(options)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;31m# Delete the training file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mcall_megam\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'args should be a list of strings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_megam_bin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mconfig_megam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Call megam via a subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/classify/megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[0;34m(bin)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MEGAM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'megam.opt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_686'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'megam_i686.opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         url='http://www.umiacs.umd.edu/~hal/megam/index.html')\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-82c2b9011d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ChunkParse score:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     IOB Accuracy:  96.0%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     Precision:     88.6%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     Recall:        91.0%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunker' is not defined"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(test_sents))\n",
    "# ChunkParse score:\n",
    "#     IOB Accuracy:  96.0%\n",
    "#     Precision:     88.6%\n",
    "#     Recall:        91.0%\n",
    "#     F-Measure:     89.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
