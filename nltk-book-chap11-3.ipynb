{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Natural Language Processing with Python â€“ Analyzing Text with the Natural Language Toolkit\n",
    "Steven Bird, Ewan Klein, and Edward Loper\n",
    "http://www.nltk.org/book/\n",
    "Chapter 11. Managing Linguistic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11.3 Acquiring Data\n",
    "11.3.1 Obtaining Data from the Web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11.3.2 Obtaining Data from Word Processor Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # added\n",
    "pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dict.htm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c468face99b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dict.htm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"windows-1252\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dict.htm'"
     ]
    }
   ],
   "source": [
    "document = open(\"dict.htm\", encoding=\"windows-1252\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_pos = set(re.findall(pattern, document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illegal_pos = used_pos.difference(legal_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(illegal_pos))\n",
    "# ['v.i', 'intrans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def lexical_data(html_file, encoding=\"utf-8\"):\n",
    "    SEP = '_ENTRY'\n",
    "    html = open(html_file, encoding=encoding).read()\n",
    "    html = re.sub(r'<p', SEP + '<p', html)\n",
    "    text = BeautifulSoup(html).get_text()\n",
    "    text = ' '.join(text.split())\n",
    "    for entry in text.split(SEP):\n",
    "        if entry.count(' ') > 2:\n",
    "            yield entry.split(' ', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = csv.writer(open(\"dict1.csv\", \"w\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.writerows(lexical_data(\"dict.htm\", encoding=\"windows-1252\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11.3.3 Obtaining Data from Spreadsheets and Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = csv.reader(open('dict.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes, defns = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defn_words = set(w for defn in defns for w in defn.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(defn_words.difference(lexemes))\n",
    "# ['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',\n",
    "# 'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11.3.4 Converting Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-479a0886d983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m idx = nltk.Index((defn_word, lexeme)\n\u001b[0;32m----> 2\u001b[0;31m  \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlexeme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mdefn_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  if len(defn_word) > 3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "idx = nltk.Index((defn_word, lexeme)\n",
    " for (lexeme, defn) in pairs\n",
    " for defn_word in nltk.word_tokenize(defn)\n",
    " if len(defn_word) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dict.idx\", \"w\") as idx_file:\n",
    "    for word in sorted(idx):\n",
    "        idx_words = ', '.join(idx[word])\n",
    "        idx_line = \"{}: {}\".format(word, idx_words)\n",
    "        print(idx_line, file=idx_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11.3.5 Deciding Which Layers of Annotation to Include\n",
    "11.3.6 Standards and Tools"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11.3.7 Special Considerations when Working with Endangered Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),\n",
    "            ('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(word):\n",
    "    for patt, repl in mappings:\n",
    "        word = re.sub(patt, repl, word)\n",
    "    pieces = re.findall('[^aeiou]+', word)\n",
    "    return ''.join(char for piece in pieces for char in sorted(piece))[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lfnt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature('illefent')\n",
    "# 'lfnt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bskws'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature('ebsekwieous')\n",
    "# 'bskws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nclr'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature('nuculerr')\n",
    "# 'nclr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anicular',\n",
       " 'inocular',\n",
       " 'nucellar',\n",
       " 'nuclear',\n",
       " 'unicolor',\n",
       " 'uniocular',\n",
       " 'unocular']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signatures[signature('nuculerr')]\n",
    "# ['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(word, wordlist):\n",
    "    ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)\n",
    "    return [word for (_, word) in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_spell(word):\n",
    "    sig = signature(word)\n",
    "    if sig in signatures:\n",
    "        return rank(word, signatures[sig])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olefiant', 'elephant', 'oliphant', 'elephanta']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_spell('illefent')\n",
    "# ['olefiant', 'elephant', 'oliphant', 'elephanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obsequious']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_spell('ebsekwieous')\n",
    "# ['obsequious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anicular',\n",
       " 'inocular',\n",
       " 'nucellar',\n",
       " 'nuclear',\n",
       " 'unocular',\n",
       " 'uniocular',\n",
       " 'unicolor']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_spell('nucular')\n",
    "# ['anicular', 'inocular', 'nucellar', 'nuclear', 'unocular', 'uniocular', 'unicolor']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
